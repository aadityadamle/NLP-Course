{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AdvancedNLPTasks (Demo 4)",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aadityadamle/NLP-Course/blob/main/AdvancedNLPTasks_(Demo_4).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzeo7vfKH-4x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "097eb4cd-48ae-40dc-9470-590c4b5d40e5"
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"book\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViXz7eAxydyu"
      },
      "source": [
        "#Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st7aLzo6vgn2"
      },
      "source": [
        "Tokenization is the task of cutting a string into smaller units. Blocks of text can be tokenized into sentences and even words in a particular language. This process can be done using basic regular expression and dedicated nltk functions as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5N5_rLiup1Z"
      },
      "source": [
        "#Tokenization\n",
        "#Using Regular expressions - sample text\n",
        "text = '''Although Gainesville  isn’t really a ‘big city’, it’s certainly not a \n",
        "          tiny village either. It was recently voted the 9th best college town \n",
        "          in America, perhaps because of its extremely student-friendly nature. \n",
        "          There’s enough  to do, and it’s pretty self-sufficient, so to speak. \n",
        "          Of course, if you need a breather, you could always make the hour long \n",
        "          journeys to Jacksonville or Orlando, and Miami is never too far.'''"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5V5C3YM17Bw"
      },
      "source": [
        "Using Regular Expressions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJunpN0nvp9I"
      },
      "source": [
        "Regular Expressions are patterns which can be matched to string to do a variety of tasks. Here we are using it to split the string into two parts by matching spaces or newline characters to obtain a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdCGLcivu802"
      },
      "source": [
        "#import regular expressions\n",
        "import re"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1_zvVzjxKY_",
        "outputId": "a8270ed2-6f35-41fe-e9fd-6af46039a819"
      },
      "source": [
        "#Splitting on spaces (\" \")\n",
        "print(re.split(r' ',text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Although', 'Gainesville', '', 'isn’t', 'really', 'a', '‘big', 'city’,', 'it’s', 'certainly', 'not', 'a', '\\n', '', '', '', '', '', '', '', '', '', 'tiny', 'village', 'either.', 'It', 'was', 'recently', 'voted', 'the', '9th', 'best', 'college', 'town', '\\n', '', '', '', '', '', '', '', '', '', 'in', 'America,', 'perhaps', 'because', 'of', 'its', 'extremely', 'student-friendly', 'nature.', '\\n', '', '', '', '', '', '', '', '', '', 'There’s', 'enough', '', 'to', 'do,', 'and', 'it’s', 'pretty', 'self-sufficient,', 'so', 'to', 'speak.', '\\n', '', '', '', '', '', '', '', '', '', 'Of', 'course,', 'if', 'you', 'need', 'a', 'breather,', 'you', 'could', 'always', 'make', 'the', 'hour', 'long', '\\n', '', '', '', '', '', '', '', '', '', 'journeys', 'to', 'Jacksonville', 'or', 'Orlando,', 'and', 'Miami', 'is', 'never', 'too', 'far.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdqd1HGSv5dB"
      },
      "source": [
        "To avoid newlines and tabspaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k45EsGBdvBOc",
        "outputId": "c9e345d6-68f2-4190-8291-4088b8a1d492"
      },
      "source": [
        "#Recognize tabspaces and newline character\n",
        "print(re.split(r'[ \\t\\n]+',text))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Although', 'Gainesville', 'isn’t', 'really', 'a', '‘big', 'city’,', 'it’s', 'certainly', 'not', 'a', 'tiny', 'village', 'either.', 'It', 'was', 'recently', 'voted', 'the', '9th', 'best', 'college', 'town', 'in', 'America,', 'perhaps', 'because', 'of', 'its', 'extremely', 'student-friendly', 'nature.', 'There’s', 'enough', 'to', 'do,', 'and', 'it’s', 'pretty', 'self-sufficient,', 'so', 'to', 'speak.', 'Of', 'course,', 'if', 'you', 'need', 'a', 'breather,', 'you', 'could', 'always', 'make', 'the', 'hour', 'long', 'journeys', 'to', 'Jacksonville', 'or', 'Orlando,', 'and', 'Miami', 'is', 'never', 'too', 'far.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOP89YKev_wN"
      },
      "source": [
        "In the following code many patterns are used to match one them with the string for tokenization. The patterns independently recognize certain types of words like words connected with hyphens, currencies, percentages and some symbols. The text variable contains a sample of text which contains such words.\n",
        "\n",
        "Sample : Apple's world-wide[hyphen] annual revenue totaled $274.5[currency] billion for the 2020 fiscal year. Apple is the world's largest technology company by revenue([symbol]income)[symbol] and one of the world's most valuable companies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgFdHySDwu6s"
      },
      "source": [
        "#Using regexp_tokenize function - Sample text\n",
        "text = '''Apple's world-wide annual revenue totaled $274.5 billion for the 2020 \n",
        "          fiscal year. Apple is the world's largest technology company by \n",
        "          revenue(income) and one of the world's most valuable companies.'''"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErM92GKJxQc7"
      },
      "source": [
        "#Setting rules\n",
        "rules = r'''(?x)     # set flag to allow verbose regexps\n",
        "  \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "| \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $274.5\n",
        "| [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAQXGNSXxUKk",
        "outputId": "54f03b3e-e6fd-4dad-f269-65831e502034"
      },
      "source": [
        "#Using the function\n",
        "print(nltk.regexp_tokenize(text, rules))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple', \"'\", 's', 'world-wide', 'annual', 'revenue', 'totaled', '$274.5', 'billion', 'for', 'the', '2020', 'fiscal', 'year', '.', 'Apple', 'is', 'the', 'world', \"'\", 's', 'largest', 'technology', 'company', 'by', 'revenue', '(', 'income', ')', 'and', 'one', 'of', 'the', 'world', \"'\", 's', 'most', 'valuable', 'companies', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-__IdEoJwtRu"
      },
      "source": [
        "The nltk also contains in-built word tokenizer for ease of use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4I3KMqnty7if",
        "outputId": "31901f77-850d-49d2-beba-4fe83cdc0154"
      },
      "source": [
        "#Using in-built tokenizer\n",
        "tokens = nltk.word_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Apple', \"'s\", 'world-wide', 'annual', 'revenue', 'totaled', '$', '274.5', 'billion', 'for', 'the', '2020', 'fiscal', 'year', '.', 'Apple', 'is', 'the', 'world', \"'s\", 'largest', 'technology', 'company', 'by', 'revenue', '(', 'income', ')', 'and', 'one', 'of', 'the', 'world', \"'s\", 'most', 'valuable', 'companies', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5XZUZ4Pw0Gw"
      },
      "source": [
        "You can easily discern the difference between results from in-built tokenizer and our user - defined tokenizer. The words like \"'s\" and \"$\" are identified separately with the help of in-built tokenizer. Obviously the in-built tokenizer will show better performance but we have shown how you can define a tokenizer according to your needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_ws0-2w5nN"
      },
      "source": [
        "To tokenize sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjLsbeyVzjrB",
        "outputId": "1591da89-d459-4360-e63f-81afba840758"
      },
      "source": [
        "#Sentence tokenizer\n",
        "para = '''We all are going through a pandemic and because of the spread of this coronavirus, more than a million people have already lost their lives across the world. Economy is affected because of lockdowns. With Unlock now in effect, it is our responsibility to take precautions so that another cycle does not start. Everyone is accepting new habits like wearing a mask, using sanitizer etc. as a new normal. In this huge society, you do find exceptions - people not wearing masks may come close enough to you.'''\n",
        "\n",
        "sentences = nltk.sent_tokenize(para)\n",
        "sentences"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['We all are going through a pandemic and because of the spread of this coronavirus, more than a million people have already lost their lives across the world.',\n",
              " 'Economy is affected because of lockdowns.',\n",
              " 'With Unlock now in effect, it is our responsibility to take precautions so that another cycle does not start.',\n",
              " 'Everyone is accepting new habits like wearing a mask, using sanitizer etc.',\n",
              " 'as a new normal.',\n",
              " 'In this huge society, you do find exceptions - people not wearing masks may come close enough to you.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjqCJeVuyPu7"
      },
      "source": [
        "#Normalization and Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW1YgD3yxDQU"
      },
      "source": [
        "Normalizing text to lowercase can reduce the size of vocabulary by a considerable amount. We can also perform stemming to normalize the text further more by removing the affixes.\n",
        "\n",
        "Stemming is process of converting all the forms of a word to its base form. I have framed two sentences using many forms of the word \"run\" and \"tired\" to explain it the stemming process. I am marking the forms here in parathesis.\n",
        "\n",
        "Sentence1 : Though Joey was tired of running[noun form] the whole day, he still ran[past tense of \"run\"] as fast as he could to escaple the chasing beast. As he was a marathon runner[a person that runs], he was always ready for a long run[the action] but refraining from food for past 2 days had taken a toll on his muscles.\n",
        "\n",
        "Sentence2 : All the players looked very tired[past tense of \"tire\"] on the very second day of the game. Although the Test Cricket is a very tiring[the action (adjective)] format for players but this time the cause of such tiredness[physical state] was the hot, dry climate of the venue and not the format itself as a physical conditioning team prepares players to play in the Test format without getting tired[past tense of \"tire\"].\n",
        "\n",
        "The sentences contains multiple forms of words \"run\" and \"tired\". By applying stemming we will convert it to one single form and thus normalize all the words related to the action to only one word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COK41wec0Adw"
      },
      "source": [
        "#Normalization and Stemming - Sample text\n",
        "text1 = '''Though Joey was tired of running the whole day, he still ran as fast \n",
        "as he could to escape the chasing beast. As he was a marathon runner, he was \n",
        "always ready for a long run but refraining from food for past 2 days had taken \n",
        "a toll on his muscles.'''\n",
        "\n",
        "text2 = '''All the players looked very tired on the very second day of the game. \n",
        "Although the Test Cricket is a very tiring format for players but this time the \n",
        "cause of such tiredness was the hot, dry climate of the venue and not the format\n",
        "itself as a physical conditioning team prepares players to play in the Test \n",
        "format without getting tired.'''"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRBttlF70f-P"
      },
      "source": [
        "#Import and use tokenizer\n",
        "from nltk import word_tokenize\n",
        "tokens1 = word_tokenize(text1)\n",
        "tokens2 = word_tokenize(text2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4wJwOjRxOYE"
      },
      "source": [
        "The Porter and Lancaster stemmers follow their own rules for stripping affixes. The Porter Stemmer is a good choice if you are indexing some texts and want to support search using alternative forms of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtxUY2zSxVNB"
      },
      "source": [
        "Using Porter Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6WwrKXqxbdx"
      },
      "source": [
        "It is the most commonly used stemmer, also one of the most gentle stemmers. One of the few stemmers that actually has Java support, though it is also the most computationally intensive of the algorithms. It is also the oldest stemming algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTL4eViy0pwI"
      },
      "source": [
        "from nltk import PorterStemmer \n",
        "#create porterstemmer object\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Fm3XYcHhUq",
        "outputId": "e08cbfee-3a08-4b6c-ad6f-1ed38e3ff108"
      },
      "source": [
        "print([ps.stem(token) for token in tokens1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['though', 'joey', 'wa', 'tire', 'of', 'run', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escap', 'the', 'chase', 'beast', '.', 'As', 'he', 'wa', 'a', 'marathon', 'runner', ',', 'he', 'wa', 'alway', 'readi', 'for', 'a', 'long', 'run', 'but', 'refrain', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'taken', 'a', 'toll', 'on', 'hi', 'muscl', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDMuzrLRxijA"
      },
      "source": [
        "As you can see in the output some forms of the word \"run\" are still not coverted. This depends on the stemming algorithm used. We will try using another algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQaeiS6TBAuB",
        "outputId": "9da32644-ec88-4769-d173-2a0632869a31"
      },
      "source": [
        "print([ps.stem(token) for token in tokens2])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['all', 'the', 'player', 'look', 'veri', 'tire', 'on', 'the', 'veri', 'second', 'day', 'of', 'the', 'game', '.', 'although', 'the', 'test', 'cricket', 'is', 'a', 'veri', 'tire', 'format', 'for', 'player', 'but', 'thi', 'time', 'the', 'caus', 'of', 'such', 'tired', 'wa', 'the', 'hot', ',', 'dri', 'climat', 'of', 'the', 'venu', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physic', 'condit', 'team', 'prepar', 'player', 'to', 'play', 'in', 'the', 'test', 'format', 'without', 'get', 'tire', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlhgK9dwxjqx"
      },
      "source": [
        "Using Lancaster Stemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8WsQRNWxoDB"
      },
      "source": [
        "It is a very aggressive stemming algorithm. With Lancaster, the stemmed representations are considerably unintuitive to the reader as many shorter words will become totally obfuscated. Its the fastest algorithm here and will reduce your working set of words hugely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuAKBCG2HhPD"
      },
      "source": [
        "from nltk import LancasterStemmer\n",
        "#create lancasterstemmer object\n",
        "ls = LancasterStemmer()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAEPwfADHhFe",
        "outputId": "eaf0b019-06d1-4e7b-e5e6-416079c4fc91"
      },
      "source": [
        "print([ls.stem(token) for token in tokens1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['though', 'joey', 'was', 'tir', 'of', 'run', 'the', 'whol', 'day', ',', 'he', 'stil', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escap', 'the', 'chas', 'beast', '.', 'as', 'he', 'was', 'a', 'marathon', 'run', ',', 'he', 'was', 'alway', 'ready', 'for', 'a', 'long', 'run', 'but', 'refrain', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'tak', 'a', 'tol', 'on', 'his', 'musc', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re9vBmXSxx8t"
      },
      "source": [
        "Now all the forms except \"ran\" are converted to run successfully. You can see that some words in the output are not complete. This is the drawback of stemming that it converts words to base form while the base form may or may not be a meaningful word. To solve this, we use lemmatizers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUArv91yBLO0",
        "outputId": "d5a9fb8a-9d27-47ba-dcf2-fce29aa2881b"
      },
      "source": [
        "print([ls.stem(token) for token in tokens2])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['al', 'the', 'play', 'look', 'very', 'tir', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'gam', '.', 'although', 'the', 'test', 'cricket', 'is', 'a', 'very', 'tir', 'form', 'for', 'play', 'but', 'thi', 'tim', 'the', 'caus', 'of', 'such', 'tir', 'was', 'the', 'hot', ',', 'dry', 'clim', 'of', 'the', 'venu', 'and', 'not', 'the', 'form', 'itself', 'as', 'a', 'phys', 'condit', 'team', 'prep', 'play', 'to', 'play', 'in', 'the', 'test', 'form', 'without', 'get', 'tir', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TObiKnziyKEZ"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTMkGPdwx9bc"
      },
      "source": [
        "Lemmatization is a process just like stemming but the resulting form is a known word in a dictionary. This can be confirmed by comparing the output of stemmers and lemmatizers, The words \"tired\", \"still\" and \"escape\" are good examples to see the difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMUfdGhux_s6"
      },
      "source": [
        "Using WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKlJSor_yDDi"
      },
      "source": [
        "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower. The WordNet lemmatizer is a good choice if you want to compile the vocabulary of some texts and want a list of valid lemmas (or lexicon headwords)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOZZAoMNHnlv"
      },
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "#create lemmatizer object\n",
        "WordNet = WordNetLemmatizer()"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Miifyh9HnjW",
        "outputId": "96517c54-a671-455f-b916-fcecb8edbe16"
      },
      "source": [
        "print([WordNet.lemmatize(token) for token in tokens1])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Though', 'Joey', 'wa', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'a', 'fast', 'a', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'wa', 'a', 'marathon', 'runner', ',', 'he', 'wa', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscle', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUSUQpG8BRc8",
        "outputId": "d11b68ad-bb05-487b-ef40-15a36741b4ea"
      },
      "source": [
        "print([WordNet.lemmatize(token) for token in tokens2])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['All', 'the', 'player', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'player', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'wa', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'a', 'a', 'physical', 'conditioning', 'team', 'prepares', 'player', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}